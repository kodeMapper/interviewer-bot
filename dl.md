# ğŸ§  Deep Learning Implementation Documentation
## AI Smart Interviewer & Proctoring System

**Course:** Deep Learning  
**Student Project Documentation**  
**Date:** January 2026

---

## ğŸ“‘ Table of Contents

1. [Project Overview](#1-project-overview)
2. [Architecture Overview](#2-architecture-overview)
3. [Module-wise Syllabus Mapping](#3-module-wise-syllabus-mapping)
   - [Module 1: Introduction to Deep Learning](#module-1-introduction-to-deep-learning)
   - [Module 2: Shallow Neural Networks](#module-2-shallow-neural-networks)
   - [Module 3: Optimization Algorithms](#module-3-optimization-algorithms)
   - [Module 4: Convolutional Neural Networks](#module-4-convolutional-neural-networks)
   - [Module 5: Autoencoders](#module-5-autoencoders)
   - [Module 6: Regularization](#module-6-regularization)
4. [Mathematical Foundations](#4-mathematical-foundations)
5. [Implementation Deep Dive](#5-implementation-deep-dive)
6. [Training Pipeline](#6-training-pipeline)
7. [Evaluation Metrics](#7-evaluation-metrics)
8. [Code-to-Concept Mapping](#8-code-to-concept-mapping)
9. [Viva Questions & Answers](#9-viva-questions--answers)
10. [References](#10-references)

---

## 1. Project Overview

### 1.1 What We Built

This project implements an **AI-powered Smart Interview System** that uses Deep Learning to:

1. **Understand Candidate Answers** - Classify user responses into technical topics using a custom-trained Multi-Layer Perceptron (MLP)
2. **Semantic Answer Evaluation** - Evaluate answer quality using sentence embeddings and cosine similarity
3. **Adaptive Question Generation** - Dynamically route questions based on detected topics

### 1.2 Why This is a "White Box" Deep Learning Project

Unlike wrapper applications that simply call pre-built APIs, this project demonstrates:

- âœ… **Custom Neural Network Architecture** - Designed and implemented from scratch using PyTorch
- âœ… **Manual Training Loop** - Full implementation of forward pass, loss computation, and backpropagation
- âœ… **Optimizer Comparison** - Comparative analysis of SGD vs Adam optimizers
- âœ… **Regularization Techniques** - Dropout, L2 regularization, early stopping
- âœ… **Vectorization** - Text-to-embedding transformation using sentence transformers
- âœ… **Evaluation Metrics** - Precision, Recall, F1 Score, Multi-label accuracy

### 1.3 Project Components with Deep Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI SMART INTERVIEWER SYSTEM                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   INTENT         â”‚   â”‚   ANSWER         â”‚   â”‚   QUESTION       â”‚   â”‚
â”‚  â”‚   CLASSIFIER     â”‚   â”‚   EVALUATOR      â”‚   â”‚   ROUTER         â”‚   â”‚
â”‚  â”‚   (MLP Brain)    â”‚   â”‚   (Embeddings)   â”‚   â”‚   (Priority Q)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                      â”‚                      â”‚              â”‚
â”‚           â–¼                      â–¼                      â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              DEEP LEARNING COMPONENTS                          â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚  â€¢ Multi-Layer Perceptron (MLP)     [Module 1, 2]             â”‚   â”‚
â”‚   â”‚  â€¢ Backpropagation Training         [Module 2]                â”‚   â”‚
â”‚   â”‚  â€¢ Adam/SGD Optimization            [Module 3]                â”‚   â”‚
â”‚   â”‚  â€¢ Dropout Regularization           [Module 6]                â”‚   â”‚
â”‚   â”‚  â€¢ Sentence Embeddings (384-dim)    [Vectorization]           â”‚   â”‚
â”‚   â”‚  â€¢ Multi-Label Classification       [Binary Classification]  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Architecture Overview

### 2.1 System Architecture Diagram

```
                              USER INPUT (Voice/Text)
                                       â”‚
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Whisper ASR (STT)      â”‚
                        â”‚   Audio â†’ Text           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Sentence Transformer    â”‚
                        â”‚  Text â†’ 384-dim Vector   â”‚ â—„â”€â”€ VECTORIZATION (Module 1)
                        â”‚  (all-MiniLM-L6-v2)      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                          x âˆˆ â„^384 (embedding)
                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                     â”‚
                    â–¼                                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   INTENT CLASSIFIER   â”‚           â”‚   ANSWER EVALUATOR    â”‚
        â”‚   (Custom MLP)        â”‚           â”‚   (Cosine Similarity) â”‚
        â”‚                       â”‚           â”‚                       â”‚
        â”‚   384 â†’ 128 â†’ 64 â†’ 7  â”‚           â”‚   sim(u,v) = uÂ·v      â”‚
        â”‚       â†“     â†“     â†“   â”‚           â”‚            â”€â”€â”€â”€â”€      â”‚
        â”‚     ReLU  ReLU  Sig   â”‚           â”‚            |u||v|     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                     â”‚
                    â–¼                                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Topic Probabilities  â”‚           â”‚  Semantic Score (%)   â”‚
        â”‚  [Java: 0.9, ...]     â”‚           â”‚  [0-100]              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   INTERVIEW CONTROLLER   â”‚
                        â”‚   Adaptive Question      â”‚
                        â”‚   Selection              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                              NEXT QUESTION
```

### 2.2 Neural Network Architecture

Our Intent Classifier is a **Multi-Layer Perceptron (MLP)** with the following structure:

```
INPUT LAYER                 HIDDEN LAYER 1              HIDDEN LAYER 2              OUTPUT LAYER
   (384)                       (128)                       (64)                        (7)
                                                                                     
  â”Œâ”€â”€â”€â”                       â”Œâ”€â”€â”€â”                       â”Œâ”€â”€â”€â”                      â”Œâ”€â”€â”€â”
  â”‚xâ‚ â”‚â”€â”€â”                 â”Œâ”€â–¶â”‚hâ‚ â”‚â”€â”€â”                 â”Œâ”€â–¶â”‚hâ‚'â”‚â”€â”€â”                â”Œâ”€â–¶â”‚yâ‚ â”‚ Java
  â”œâ”€â”€â”€â”¤  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”œâ”€â”€â”€â”¤
  â”‚xâ‚‚ â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚ Wâ‚Â·x+bâ‚ â”‚â”€â”€â”¼â”€â–¶â”‚hâ‚‚ â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚ Wâ‚‚Â·h+bâ‚‚ â”‚â”€â”€â”¼â”€â–¶â”‚hâ‚‚'â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚ Wâ‚ƒÂ·h'+bâ‚ƒâ”‚â”€â”€â”¼â”€â–¶â”‚yâ‚‚ â”‚ Python
  â”œâ”€â”€â”€â”¤  â”‚    â”‚  ReLU   â”‚  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â”‚  ReLU   â”‚  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â”‚ Sigmoid â”‚  â”‚  â”œâ”€â”€â”€â”¤
  â”‚xâ‚ƒ â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚ Dropout â”‚â”€â”€â”¼â”€â–¶â”‚hâ‚ƒ â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚ Dropout â”‚â”€â”€â”¼â”€â–¶â”‚hâ‚ƒ'â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚         â”‚â”€â”€â”¼â”€â–¶â”‚yâ‚ƒ â”‚ JavaScript
  â”œâ”€â”€â”€â”¤  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€â”€â”€â”¤  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€â”€â”€â”¤
  â”‚...â”‚  â”‚                 â”‚  â”‚...â”‚  â”‚                 â”‚  â”‚...â”‚  â”‚                 â”‚  â”‚yâ‚„ â”‚ React
  â”œâ”€â”€â”€â”¤  â”‚                 â”‚  â”œâ”€â”€â”€â”¤  â”‚                 â”‚  â”œâ”€â”€â”€â”¤  â”‚                 â”‚  â”œâ”€â”€â”€â”¤
  â”‚xâ‚ƒâ‚ˆâ‚„â”‚â”€â”˜                 â””â”€â–¶â”‚hâ‚â‚‚â‚ˆâ”‚â”€â”˜                 â””â”€â–¶â”‚hâ‚†â‚„â”‚â”€â”˜                 â””â”€â–¶â”‚yâ‚‡ â”‚ Deep_Learning
  â””â”€â”€â”€â”˜                       â””â”€â”€â”€â”˜                       â””â”€â”€â”€â”˜                      â””â”€â”€â”€â”˜
                                                                                     
  Embeddings                   ReLU + Dropout(0.3)        ReLU + Dropout(0.3)         Sigmoid
```

**Architecture Summary:**

| Layer | Input Dim | Output Dim | Activation | Regularization |
|-------|-----------|------------|------------|----------------|
| Input | 384 | 384 | - | - |
| Hidden 1 | 384 | 128 | ReLU | Dropout (0.3) |
| Hidden 2 | 128 | 64 | ReLU | Dropout (0.3) |
| Output | 64 | 7 | Sigmoid | - |

**Total Parameters:** 55,623 trainable parameters

---

## 3. Module-wise Syllabus Mapping

### Module 1: Introduction to Deep Learning

**(4 Hours) - Perceptrons, MLPs, Vectorization**

#### 1.1 Concepts Implemented

| Syllabus Topic | Implementation | File Location |
|----------------|----------------|---------------|
| Perceptron | Single Linear Layer (`nn.Linear`) | `intent_classifier.py` |
| McCulloch-Pitts Neuron | Binary threshold activation | Conceptual basis for ReLU |
| Multi-Layer Perceptrons | 3-layer MLP architecture | `intent_classifier.py` |
| Sigmoid Neurons | Output layer activation | `intent_classifier.py` |
| Binary Classification | Multi-label classification (7 independent binary classifiers) | `train_intent_model.py` |
| Vectorization | Text â†’ 384-dim embeddings using Sentence Transformers | `data_loader.py` |
| Computation Graph | PyTorch autograd builds dynamic computation graphs | Training loop |

#### 1.2 Code Implementation

**File:** `backend/ml/models/intent_classifier.py`

```python
class IntentClassifier(nn.Module):
    """
    Multi-Layer Perceptron for Multi-Label Intent Classification.
    
    Architecture:
        Input (384) -> Linear(128) -> ReLU -> Dropout
                    -> Linear(64) -> ReLU -> Dropout  
                    -> Linear(7) -> Sigmoid (for multi-label)
    """
    
    def __init__(
        self, 
        input_dim: int = 384,      # Sentence-Transformer embedding size
        hidden_dim_1: int = 128,   # First hidden layer
        hidden_dim_2: int = 64,    # Second hidden layer
        output_dim: int = 7,       # Number of topic classes
        dropout_rate: float = 0.3  # Dropout for regularization (Module 6)
    ):
        super(IntentClassifier, self).__init__()
        
        # Define layers - MULTI-LAYER PERCEPTRON
        self.network = nn.Sequential(
            # Layer 1: Input -> Hidden 1 (PERCEPTRON WITH ACTIVATION)
            nn.Linear(input_dim, hidden_dim_1),  # Wâ‚Â·x + bâ‚
            nn.ReLU(),  # Non-linear activation (Module 1)
            nn.Dropout(dropout_rate),  # Regularization (Module 6)
            
            # Layer 2: Hidden 1 -> Hidden 2
            nn.Linear(hidden_dim_1, hidden_dim_2),  # Wâ‚‚Â·h + bâ‚‚
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            # Output Layer: Hidden 2 -> Output (SIGMOID NEURONS)
            nn.Linear(hidden_dim_2, output_dim)  # Wâ‚ƒÂ·h' + bâ‚ƒ
            # Sigmoid applied via BCEWithLogitsLoss for numerical stability
        )
```

#### 1.3 Mathematical Foundations

**Perceptron Model:**

A single perceptron computes:

$$y = \sigma(w^T x + b)$$

Where:
- $x \in \mathbb{R}^n$ is the input vector
- $w \in \mathbb{R}^n$ is the weight vector
- $b \in \mathbb{R}$ is the bias term
- $\sigma$ is the activation function

**Multi-Layer Perceptron (MLP):**

For our 3-layer network:

$$h^{(1)} = \text{ReLU}(W^{(1)} x + b^{(1)})$$
$$h^{(2)} = \text{ReLU}(W^{(2)} h^{(1)} + b^{(2)})$$
$$\hat{y} = \sigma(W^{(3)} h^{(2)} + b^{(3)})$$

**Sigmoid Function (for output layer):**

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Properties:
- Range: $(0, 1)$
- Derivative: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$
- Used for probabilistic interpretation

**ReLU Function (for hidden layers):**

$$\text{ReLU}(z) = \max(0, z)$$

Properties:
- Range: $[0, \infty)$
- Derivative: $\text{ReLU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$
- Solves vanishing gradient problem

**Vectorization - Text Embeddings:**

```python
# File: backend/ml/data/data_loader.py

# Generate embeddings using Sentence Transformers
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(texts, show_progress_bar=True)
# Result: texts â†’ embeddings âˆˆ â„^(NÃ—384)
```

The embedding maps text to a 384-dimensional vector space where:
- Similar texts have similar embeddings
- Distance measures semantic similarity

#### 1.4 Viva Questions (Module 1)

**Q1: What is a Perceptron and how is it different from a Neuron in an MLP?**

*Answer:* A Perceptron is a single-layer neural network that can only solve linearly separable problems. It computes $y = \text{step}(w^T x + b)$. In an MLP, neurons have non-linear activations (ReLU, Sigmoid) and are stacked in multiple layers, enabling them to learn complex, non-linear decision boundaries.

**Q2: Why do we need multiple layers in a neural network?**

*Answer:* According to the Universal Approximation Theorem, a single hidden layer MLP can approximate any continuous function. However, deeper networks can represent the same functions with exponentially fewer neurons. In our project, the 2 hidden layers (128â†’64) allow the network to learn hierarchical features from the 384-dimensional embeddings.

**Q3: Explain the role of vectorization in your project.**

*Answer:* Vectorization converts variable-length text inputs into fixed-size numerical vectors (384 dimensions). We use Sentence Transformers (MiniLM) which produces semantically meaningful embeddings - similar texts produce similar vectors. This is essential because neural networks require numerical inputs of fixed dimensions.

**Q4: What is a Computation Graph?**

*Answer:* A computation graph is a directed graph where nodes represent operations (addition, multiplication, activation functions) and edges represent data flow (tensors). PyTorch uses dynamic computation graphs (define-by-run) which means the graph is built on-the-fly during the forward pass. This enables automatic differentiation (autograd) for computing gradients during backpropagation.

---

### Module 2: Shallow Neural Networks

**(6 Hours) - Feedforward NNs, Backpropagation**

#### 2.1 Concepts Implemented

| Syllabus Topic | Implementation | File Location |
|----------------|----------------|---------------|
| Neural Network Representation | Layer-wise architecture | `intent_classifier.py` |
| Activation Functions | ReLU (hidden), Sigmoid (output) | `intent_classifier.py` |
| Feedforward Neural Networks | Sequential model with `forward()` | `intent_classifier.py` |
| Gradient Descent | Parameter updates using `optimizer.step()` | `train_intent_model.py` |
| Backpropagation Algorithm | `loss.backward()` computes gradients | `train_intent_model.py` |

#### 2.2 Forward Pass Implementation

**File:** `backend/ml/models/intent_classifier.py`

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    Forward pass through the network.
    
    Mathematical operation:
    zâ‚ = Wâ‚Â·x + bâ‚
    hâ‚ = ReLU(zâ‚) * (1 - dropout_maskâ‚)   # With dropout during training
    zâ‚‚ = Wâ‚‚Â·hâ‚ + bâ‚‚
    hâ‚‚ = ReLU(zâ‚‚) * (1 - dropout_maskâ‚‚)
    zâ‚ƒ = Wâ‚ƒÂ·hâ‚‚ + bâ‚ƒ                        # Logits (before sigmoid)
    
    Args:
        x: Input tensor of shape (batch_size, 384)
    
    Returns:
        Logits tensor of shape (batch_size, 7)
    """
    return self.network(x)
```

#### 2.3 Training Loop with Backpropagation

**File:** `backend/ml/training/train_intent_model.py`

```python
def train_epoch(model, dataloader, criterion, optimizer, device):
    """
    Train for one epoch - Complete Backpropagation Implementation
    
    Steps:
    1. Forward Pass: Compute predictions
    2. Loss Computation: BCEWithLogitsLoss
    3. Backward Pass: Compute gradients (âˆ‚L/âˆ‚W)
    4. Optimizer Step: Update weights (W = W - Î·Â·âˆ‚L/âˆ‚W)
    """
    model.train()  # Enable dropout and batch norm training mode
    total_loss = 0
    
    for batch_x, batch_y in dataloader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # 1. FORWARD PASS
        optimizer.zero_grad()  # Clear accumulated gradients
        outputs = model(batch_x)  # Å· = f(x; W)
        
        # 2. LOSS COMPUTATION
        loss = criterion(outputs, batch_y)  # L = BCE(Å·, y)
        
        # 3. BACKWARD PASS (Backpropagation - Module 2)
        loss.backward()  # Computes âˆ‚L/âˆ‚W for all parameters
        
        # 4. OPTIMIZER STEP (Gradient Descent - Module 3)
        optimizer.step()  # W = W - Î·Â·âˆ‚L/âˆ‚W
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)
```

#### 2.4 Mathematical Foundations - Backpropagation

**Loss Function (Binary Cross-Entropy):**

For multi-label classification with $K=7$ classes:

$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} \left[ y_k^{(i)} \log(\hat{y}_k^{(i)}) + (1-y_k^{(i)}) \log(1-\hat{y}_k^{(i)}) \right]$$

**Backpropagation - Chain Rule:**

For output layer weights $W^{(3)}$:

$$\frac{\partial \mathcal{L}}{\partial W^{(3)}} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}} \cdot \frac{\partial z^{(3)}}{\partial W^{(3)}}$$

Where:
- $\frac{\partial \mathcal{L}}{\partial \hat{y}} = \hat{y} - y$ (gradient of BCE w.r.t. output)
- $\frac{\partial \hat{y}}{\partial z^{(3)}} = \sigma(z^{(3)})(1-\sigma(z^{(3)}))$ (sigmoid derivative)
- $\frac{\partial z^{(3)}}{\partial W^{(3)}} = h^{(2)}$ (input to the layer)

For hidden layer weights (propagating error backwards):

$$\delta^{(l)} = (\delta^{(l+1)} \cdot W^{(l+1)T}) \odot f'(z^{(l)})$$

$$\frac{\partial \mathcal{L}}{\partial W^{(l)}} = h^{(l-1)T} \cdot \delta^{(l)}$$

**ReLU Derivative in Backpropagation:**

$$\frac{\partial \text{ReLU}(z)}{\partial z} = \mathbb{1}_{z > 0}$$

This creates a "gradient highway" - gradients flow unchanged through positive activations.

#### 2.5 Gradient Flow Visualization

```
FORWARD PASS (â†’)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
   x        zâ‚       hâ‚       zâ‚‚       hâ‚‚       zâ‚ƒ       Å·       L
   â”‚         â”‚        â”‚        â”‚        â”‚        â”‚        â”‚        â”‚
   â–¼         â–¼        â–¼        â–¼        â–¼        â–¼        â–¼        â–¼
 Input â†’ Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Sigmoid â†’ BCE Loss

â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                        BACKWARD PASS (â†)
   âˆ‚L/âˆ‚x   âˆ‚L/âˆ‚Wâ‚  âˆ‚L/âˆ‚hâ‚  âˆ‚L/âˆ‚Wâ‚‚  âˆ‚L/âˆ‚hâ‚‚  âˆ‚L/âˆ‚Wâ‚ƒ   âˆ‚L/âˆ‚Å·    âˆ‚L/âˆ‚L=1
```

#### 2.6 Viva Questions (Module 2)

**Q1: Explain the backpropagation algorithm as implemented in your project.**

*Answer:* Backpropagation computes gradients of the loss with respect to all weights using the chain rule. In PyTorch, `loss.backward()` traverses the computation graph backwards, computing $\frac{\partial \mathcal{L}}{\partial W}$ for each layer. Starting from the output, error signals propagate through: (1) Sigmoid â†’ (2) Linear â†’ (3) ReLU â†’ (4) Linear â†’ (5) ReLU â†’ (6) Linear. Each layer's gradient depends on the gradient from the next layer.

**Q2: What is the role of `optimizer.zero_grad()` in training?**

*Answer:* PyTorch accumulates gradients by default (useful for gradient accumulation). `optimizer.zero_grad()` clears the accumulated gradients before each batch. Without this, gradients would keep adding up across batches, leading to incorrect weight updates. This is called before the forward pass in each iteration.

**Q3: Why do we use BCEWithLogitsLoss instead of BCELoss?**

*Answer:* `BCEWithLogitsLoss` combines Sigmoid and BCE into a single operation using the log-sum-exp trick for numerical stability:

$$\mathcal{L} = \max(z, 0) - z \cdot y + \log(1 + e^{-|z|})$$

This avoids computing $\log(0)$ or overflow issues. The model outputs logits (before sigmoid), and the loss function applies sigmoid internally.

**Q4: What is the "representation power" of your feedforward network?**

*Answer:* Our network with 2 hidden layers (128, 64 neurons) can represent complex non-linear decision boundaries in the 384-dimensional embedding space. According to the Universal Approximation Theorem, even a single hidden layer with enough neurons can approximate any continuous function. Our architecture balances expressiveness with regularization (dropout) to generalize well.

---

### Module 3: Optimization Algorithms

**(6 Hours) - GD, Momentum, Adam, RMSProp**

#### 3.1 Concepts Implemented

| Syllabus Topic | Implementation | File Location |
|----------------|----------------|---------------|
| Gradient Descent (GD) | Base optimization principle | `train_intent_model.py` |
| Stochastic GD (SGD) | `optim.SGD` with mini-batches | `train_intent_model.py` |
| Momentum-based GD | `momentum=0.9` in SGD | `train_intent_model.py` |
| Adam Optimizer | `optim.Adam` with adaptive LR | `train_intent_model.py` |
| RMSProp | `optim.RMSprop` (implemented) | `train_intent_model.py` |
| Learning Rate Scheduling | `ReduceLROnPlateau` | `train_intent_model.py` |

#### 3.2 Optimizer Comparison Implementation

**File:** `backend/ml/training/train_intent_model.py`

```python
def train_model(
    model,
    train_loader,
    val_loader,
    optimizer_type: str = "adam",  # Choose optimizer for comparison
    epochs: int = 100,
    lr: float = 0.001,
    weight_decay: float = 1e-4,  # L2 Regularization (Module 6)
    ...
):
    """
    Full training loop with optimizer selection.
    
    Module 3 Coverage: Compares SGD vs Adam vs RMSProp optimizers
    """
    
    # OPTIMIZER SELECTION (Module 3: Optimization)
    if optimizer_type.lower() == "adam":
        # ADAM: Adaptive Moment Estimation
        # Combines momentum + RMSProp (adaptive learning rates)
        optimizer = optim.Adam(
            model.parameters(), 
            lr=lr,                    # Base learning rate
            weight_decay=weight_decay  # L2 regularization
        )
    elif optimizer_type.lower() == "sgd":
        # SGD with Momentum
        # Accelerates convergence in relevant direction
        optimizer = optim.SGD(
            model.parameters(), 
            lr=lr, 
            momentum=0.9,              # Momentum coefficient
            weight_decay=weight_decay
        )
    elif optimizer_type.lower() == "rmsprop":
        # RMSProp: Root Mean Square Propagation
        # Adapts learning rate per parameter
        optimizer = optim.RMSprop(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
    
    # LEARNING RATE SCHEDULER
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',      # Reduce LR when val_loss stops decreasing
        factor=0.5,      # LR = LR * 0.5
        patience=5,      # Wait 5 epochs before reducing
        verbose=True
    )
```

#### 3.3 Training Comparison Code

```python
# Main training pipeline - Optimizer comparison (Module 3)
histories = {}
best_model = None
best_optimizer = None
best_val_f1 = 0

for opt_name in ["Adam", "SGD"]:
    print(f"\n{'='*40}")
    print(f"Training with {opt_name}")
    print(f"{'='*40}")
    
    # Create fresh model for each optimizer
    model = IntentClassifier(
        input_dim=processed["embedding_dim"],
        output_dim=processed["num_classes"],
        dropout_rate=CONFIG["dropout_rate"]
    )
    
    trained_model, history = train_model(
        model,
        loaders["train"],
        loaders["val"],
        optimizer_type=opt_name,
        epochs=CONFIG["epochs"],
        lr=CONFIG["learning_rate"]
    )
    
    histories[opt_name] = history
    
    # Track best performing optimizer
    final_val_f1 = history["val_f1"][-1]
    if final_val_f1 > best_val_f1:
        best_val_f1 = final_val_f1
        best_model = trained_model
        best_optimizer = opt_name

# Result: Adam typically converges faster with better final performance
```

#### 3.4 Mathematical Foundations

**Gradient Descent:**

$$W_{t+1} = W_t - \eta \nabla_W \mathcal{L}(W_t)$$

Where $\eta$ is the learning rate.

**Stochastic Gradient Descent (SGD):**

Instead of computing gradient on full dataset:
$$W_{t+1} = W_t - \eta \nabla_W \mathcal{L}(W_t; x^{(i)}, y^{(i)})$$

We use mini-batches (batch_size=16 in our project).

**SGD with Momentum:**

$$v_t = \gamma v_{t-1} + \eta \nabla_W \mathcal{L}(W_t)$$
$$W_{t+1} = W_t - v_t$$

Where $\gamma = 0.9$ (momentum coefficient). This accelerates SGD in relevant directions and dampens oscillations.

**RMSProp (Root Mean Square Propagation):**

$$E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta) g_t^2$$
$$W_{t+1} = W_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$$

Adapts learning rate per parameter based on magnitude of recent gradients.

**Adam (Adaptive Moment Estimation):**

Combines Momentum + RMSProp:

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(First moment - momentum)}$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(Second moment - RMSProp)}$$

Bias correction:
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$

Update:
$$W_{t+1} = W_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

Default values: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$

#### 3.5 Optimizer Comparison Results

The training script generates comparison plots saved to `backend/ml/plots/training_curves.png`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTIMIZER COMPARISON RESULTS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  LOSS CURVES                          F1 SCORE CURVES               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  â•²                 â”‚              â”‚              ___   â”‚        â”‚
â”‚  â”‚   â•² Adam           â”‚              â”‚         ___/      â”‚        â”‚
â”‚  â”‚    â•²___            â”‚              â”‚     ___/   Adam   â”‚        â”‚
â”‚  â”‚        â•² SGD       â”‚              â”‚ ___/              â”‚        â”‚
â”‚  â”‚         â•²____      â”‚              â”‚â•±      SGD         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       Epochs â†’                           Epochs â†’                   â”‚
â”‚                                                                     â”‚
â”‚  Key Findings:                                                      â”‚
â”‚  â€¢ Adam converges ~2x faster than SGD                               â”‚
â”‚  â€¢ Adam achieves higher final F1 score (â‰ˆ0.95 vs â‰ˆ0.90)             â”‚
â”‚  â€¢ SGD with momentum is more stable but slower                      â”‚
â”‚  â€¢ Adam selected as best optimizer for production model             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.6 Viva Questions (Module 3)

**Q1: Compare Adam and SGD optimizers. Why did Adam perform better in your project?**

*Answer:* Adam combines the benefits of Momentum (accelerated gradient descent) and RMSProp (adaptive learning rates per parameter). For our intent classification task with 384-dimensional embeddings, different features have varying scales. Adam adapts the learning rate for each parameter based on historical gradient information. SGD with fixed learning rate struggles with this heterogeneity. Adam also uses momentum to escape local minima and converge faster.

**Q2: Explain the role of the learning rate scheduler in your training.**

*Answer:* We use `ReduceLROnPlateau` which monitors validation loss. When the loss plateaus (no improvement for 5 epochs), it reduces the learning rate by half. This allows:
1. Initial fast convergence with higher LR
2. Fine-tuning with lower LR as we approach the optimum
3. Escape from plateaus by reducing oscillations

**Q3: What is the difference between Batch Gradient Descent and Stochastic Gradient Descent?**

*Answer:* 
- **Batch GD**: Uses entire dataset to compute gradient. Slow, requires lots of memory, but stable updates.
- **SGD**: Uses single sample. Very noisy but fast and can escape local minima.
- **Mini-batch SGD** (our approach, batch_size=16): Balances between the two. Reduces variance while maintaining computational efficiency. Enables GPU parallelism.

**Q4: Explain the momentum term in SGD. Why is it set to 0.9?**

*Answer:* Momentum accumulates a weighted sum of past gradients: $v_t = \gamma v_{t-1} + \eta g_t$. With $\gamma = 0.9$, the effective number of steps being averaged is $\frac{1}{1-\gamma} = 10$ steps. This:
1. Accelerates convergence in consistent gradient directions
2. Dampens oscillations in high-curvature directions
3. Helps escape shallow local minima

**Q5: What is weight decay and how does it relate to L2 regularization?**

*Answer:* Weight decay adds a penalty term to the loss: $\mathcal{L}_{total} = \mathcal{L} + \frac{\lambda}{2}||W||_2^2$. The gradient becomes $\nabla \mathcal{L} + \lambda W$, which effectively shrinks weights towards zero. In our project, `weight_decay=1e-4` prevents large weight values, reducing overfitting. This is mathematically equivalent to L2 regularization.

---

### Module 4: Convolutional Neural Networks

**(7 Hours) - CNNs, LeNet, ResNet, Image Classification**

#### 4.1 Current Implementation Status

> **Note:** The CNN-based "Proctoring Eye" for webcam-based cheating detection is planned but not yet implemented in the current codebase.

#### 4.2 Planned Architecture (Future Work)

```
PROCTORING CNN ARCHITECTURE (Planned - LeNet Style)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input: Webcam Frame (224 Ã— 224 Ã— 3)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv2D(3â†’32, k=5, s=1, p=2)         â”‚  Output: 224 Ã— 224 Ã— 32
â”‚  ReLU                                 â”‚
â”‚  MaxPool2D(k=2, s=2)                  â”‚  Output: 112 Ã— 112 Ã— 32
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv2D(32â†’64, k=5, s=1, p=2)        â”‚  Output: 112 Ã— 112 Ã— 64
â”‚  ReLU                                 â”‚
â”‚  MaxPool2D(k=2, s=2)                  â”‚  Output: 56 Ã— 56 Ã— 64
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv2D(64â†’128, k=3, s=1, p=1)       â”‚  Output: 56 Ã— 56 Ã— 128
â”‚  BatchNorm2D                          â”‚
â”‚  ReLU                                 â”‚
â”‚  MaxPool2D(k=2, s=2)                  â”‚  Output: 28 Ã— 28 Ã— 128
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
       Flatten: 28 Ã— 28 Ã— 128 = 100,352
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linear(100352 â†’ 512)                 â”‚
â”‚  ReLU + Dropout(0.5)                  â”‚
â”‚  Linear(512 â†’ 2)                      â”‚  "Looking" vs "Not Looking"
â”‚  Softmax                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.3 Mathematical Foundations

**Convolution Operation:**

For input $I$ and kernel $K$:

$$S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i+m, j+n) K(m,n)$$

**Output Size Calculation:**

$$O = \frac{W - K + 2P}{S} + 1$$

Where:
- $W$ = Input width/height
- $K$ = Kernel size
- $P$ = Padding
- $S$ = Stride

**Max Pooling:**

$$Y(i,j) = \max_{m,n \in \text{pool region}} X(i \cdot s + m, j \cdot s + n)$$

Reduces spatial dimensions while retaining important features.

#### 4.4 Connection to Current Project

While CNNs are not directly implemented, the concepts are relevant:

1. **Feature Extraction**: Like CNNs extract visual features, our Sentence Transformer extracts semantic features from text
2. **Hierarchical Learning**: CNNs learn lowâ†’high level features; our MLP learns abstract topic representations
3. **Transfer Learning**: We use pre-trained embeddings (all-MiniLM-L6-v2), similar to using pre-trained CNNs (ResNet, VGG)

#### 4.5 Viva Questions (Module 4)

**Q1: How would you design a CNN for proctoring in your interview system?**

*Answer:* I would use a LeNet/VGG-style architecture with:
1. 3-4 convolutional layers with increasing filters (32â†’64â†’128)
2. Max pooling after each conv layer for spatial reduction
3. BatchNorm for training stability
4. Fully connected layers ending in 2-class softmax
5. Binary classification: "Looking at screen" vs "Looking away"
6. Dropout (0.5) to prevent memorizing backgrounds

**Q2: Explain the relationship between CNNs and the feature extraction in your current system.**

*Answer:* Both perform feature extraction:
- **CNNs**: Learn spatial hierarchies (edgesâ†’shapesâ†’objects) through convolutions
- **Sentence Transformers**: Learn semantic hierarchies (tokensâ†’phrasesâ†’meaning) through attention

Both transform raw input into a fixed-size representation suitable for classification. Our 384-dim embeddings are analogous to CNN feature maps after global pooling.

**Q3: What is the role of padding and stride in convolutions?**

*Answer:* 
- **Padding**: Adds border pixels (usually zeros) to preserve spatial dimensions. "Same" padding maintains input size; "Valid" padding (no padding) reduces size.
- **Stride**: Step size of the kernel. Stride=1 moves pixel by pixel; Stride=2 skips every other position, halving dimensions.

---

### Module 5: Autoencoders

**(5 Hours) - Autoencoders, PCA, Denoising**

#### 5.1 Connection to Project

While autoencoders aren't directly implemented, the **embedding-based approach** relates to autoencoder principles:

**Sentence Transformer as Implicit Encoder:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SENTENCE TRANSFORMER AS ENCODER                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Text                     Latent Space          Similarityâ”‚
â”‚  "I work with React hooks"         â”‚                      â”‚    â”‚
â”‚         â”‚                          â”‚                      â–¼    â”‚
â”‚         â–¼                          â–¼                 Cosine Simâ”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Tokenizer  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ 384-dim  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Match  â”‚â”‚
â”‚  â”‚  + BERT     â”‚             â”‚ Embeddingâ”‚           â”‚  Score  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  This is conceptually similar to:                               â”‚
â”‚  â€¢ Encoder compresses text to 384 dimensions                    â”‚
â”‚  â€¢ Latent space captures semantic meaning                       â”‚
â”‚  â€¢ We use this for similarity (like AE reconstruction)          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.2 Mathematical Foundations

**Autoencoder Architecture:**

$$z = f_{enc}(x; \theta_{enc}) \quad \text{(Encoding)}$$
$$\hat{x} = f_{dec}(z; \theta_{dec}) \quad \text{(Decoding)}$$

**Loss Function:**

$$\mathcal{L}_{AE} = ||x - \hat{x}||_2^2 \quad \text{(Reconstruction Loss)}$$

**Relation to PCA:**

Linear autoencoders with MSE loss learn the same subspace as PCA:
$$z = W^T x, \quad \hat{x} = W z$$

The weights $W$ converge to principal components.

**Our Embedding Analogy:**

The 384-dimensional embedding can be seen as a "latent representation":
- **Original**: Variable-length text (infinite dimensions)
- **Encoded**: 384-dim vector (compressed representation)
- **"Decoding"**: Not reconstructing text, but comparing embeddings

#### 5.3 Viva Questions (Module 5)

**Q1: How are your text embeddings related to autoencoders?**

*Answer:* Sentence Transformers are trained with objectives similar to autoencoders:
1. They compress text into a fixed-size latent space (384 dimensions)
2. The embedding captures essential semantic information
3. Similar to how autoencoders learn data manifolds, embeddings learn semantic manifolds
4. We use the latent representation directly for classification instead of reconstruction

**Q2: What is the relationship between PCA and linear autoencoders?**

*Answer:* A linear autoencoder with:
- Single hidden layer (bottleneck)
- No activation functions
- MSE reconstruction loss

learns exactly the PCA projection. The encoder weights span the same subspace as the top-k principal components. However, autoencoders can use non-linear activations to capture more complex relationships than PCA.

---

### Module 6: Regularization

**(7 Hours) - Dropout, L2, Early Stopping, Data Augmentation**

#### 6.1 Concepts Implemented

| Syllabus Topic | Implementation | File Location |
|----------------|----------------|---------------|
| L2 Regularization | `weight_decay=1e-4` in optimizer | `train_intent_model.py` |
| Dropout | `nn.Dropout(0.3)` in hidden layers | `intent_classifier.py` |
| Early Stopping | `patience=15` epochs | `train_intent_model.py` |
| Bias-Variance Tradeoff | Architecture design | Model complexity vs. generalization |
| Hyperparameter Tuning | Learning rate, dropout rate, epochs | `train_intent_model.py` |

#### 6.2 Implementation Details

**Dropout Implementation:**

```python
# File: backend/ml/models/intent_classifier.py

self.network = nn.Sequential(
    nn.Linear(input_dim, hidden_dim_1),
    nn.ReLU(),
    nn.Dropout(dropout_rate),  # Randomly zero 30% of neurons during training
    
    nn.Linear(hidden_dim_1, hidden_dim_2),
    nn.ReLU(),
    nn.Dropout(dropout_rate),  # Forces learning of redundant representations
    
    nn.Linear(hidden_dim_2, output_dim)
)
```

**L2 Regularization (Weight Decay):**

```python
# File: backend/ml/training/train_intent_model.py

optimizer = optim.Adam(
    model.parameters(), 
    lr=0.001,
    weight_decay=1e-4  # L2 penalty: adds 0.0001 * ||W||Â² to loss
)
```

**Early Stopping:**

```python
# File: backend/ml/training/train_intent_model.py

best_val_loss = float('inf')
epochs_without_improvement = 0
patience = 15

for epoch in range(epochs):
    # Train and evaluate
    train_loss = train_epoch(...)
    val_loss, val_metrics = evaluate(...)
    
    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict().copy()  # Save best
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1
    
    # Stop if no improvement
    if epochs_without_improvement >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

# Restore best model
model.load_state_dict(best_model_state)
```

#### 6.3 Mathematical Foundations

**Dropout:**

During training, each neuron is "dropped" with probability $p$:

$$h_i = \begin{cases} 
\frac{h_i}{1-p} & \text{with probability } 1-p \\
0 & \text{with probability } p
\end{cases}$$

The $\frac{1}{1-p}$ scaling (inverted dropout) ensures expected values match at test time.

**Why it works:**
- Prevents co-adaptation of neurons
- Approximates ensemble of $2^n$ sub-networks
- Forces learning of redundant representations

**L2 Regularization:**

Modified loss:
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda \sum_i w_i^2$$

Gradient update:
$$w_i \leftarrow w_i - \eta\left(\frac{\partial \mathcal{L}}{\partial w_i} + 2\lambda w_i\right)$$

This is equivalent to weight decay: weights naturally decay towards zero.

**Bias-Variance Tradeoff:**

$$\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

```
                High Bias                    High Variance
                (Underfitting)               (Overfitting)
                     â”‚                            â”‚
                     â–¼                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Simple Model     â”‚        â”‚   Complex Model    â”‚
        â”‚   Low Capacity     â”‚        â”‚   High Capacity    â”‚
        â”‚                    â”‚        â”‚                    â”‚
        â”‚      â”€â”€â”€â”€          â”‚        â”‚   â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®      â”‚
        â”‚     /    \         â”‚        â”‚  â•±   â—‹  â—‹   â•²     â”‚
        â”‚    â—‹  â—‹   â—‹        â”‚        â”‚ â•±  â—‹      â—‹  â•²    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      
        Our Model: Balanced via Dropout + L2 + Early Stopping
```

#### 6.4 Regularization Effects in Our Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGULARIZATION TECHNIQUES APPLIED                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  TECHNIQUE           EFFECT                   HYPERPARAMETER        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Dropout (0.3)       Prevents co-adaptation   dropout_rate = 0.3   â”‚
â”‚                      Ensemble approximation                         â”‚
â”‚                                                                     â”‚
â”‚  L2 / Weight Decay   Penalizes large weights  weight_decay = 1e-4  â”‚
â”‚                      Smooth decision boundary                       â”‚
â”‚                                                                     â”‚
â”‚  Early Stopping      Prevents overtraining    patience = 15 epochs â”‚
â”‚                      Uses validation loss                           â”‚
â”‚                                                                     â”‚
â”‚  Architecture        Limited capacity         128â†’64 neurons        â”‚
â”‚                      (vs 256â†’256â†’128)                               â”‚
â”‚                                                                     â”‚
â”‚  COMBINED EFFECT:                                                   â”‚
â”‚  â€¢ Training Accuracy: ~100%                                         â”‚
â”‚  â€¢ Validation Accuracy: ~95% (not overfitting!)                     â”‚
â”‚  â€¢ Gap < 5% indicates good generalization                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.5 Viva Questions (Module 6)

**Q1: Explain how Dropout prevents overfitting in your model.**

*Answer:* Dropout randomly zeroes 30% of neurons during each forward pass:
1. **Prevents co-adaptation**: Neurons can't rely on specific other neurons
2. **Ensemble effect**: Each training step uses a different sub-network; test time averages all
3. **Redundant representations**: Network learns multiple paths to the output
4. **Implicit regularization**: Reduces effective capacity during training

**Q2: What is the difference between L1 and L2 regularization? Why did you choose L2?**

*Answer:* 
- **L1 (Lasso)**: Adds $\lambda |w|$ to loss. Produces sparse weights (some become exactly 0). Good for feature selection.
- **L2 (Ridge)**: Adds $\lambda w^2$ to loss. Shrinks all weights towards zero but none become exactly 0. Smoother solutions.

We chose L2 because:
1. All 384 embedding dimensions contain useful information
2. We want smooth decision boundaries
3. L2 is computationally simpler (differentiable everywhere)

**Q3: How does early stopping work in your training pipeline?**

*Answer:* We monitor validation loss after each epoch:
1. If `val_loss` improves, save the model state and reset patience counter
2. If `val_loss` doesn't improve for 15 consecutive epochs, stop training
3. Restore the best model state (lowest validation loss)

This prevents overfitting by stopping before the model memorizes training noise.

**Q4: Explain the bias-variance tradeoff in the context of your model architecture.**

*Answer:* 
- **High Bias (Underfitting)**: Too few parameters â†’ can't learn complex patterns
- **High Variance (Overfitting)**: Too many parameters â†’ memorizes training data

Our architecture (384â†’128â†’64â†’7) with ~55K parameters balances this:
- Enough capacity to learn topic distinctions
- Regularization (dropout, L2) prevents overfitting
- Result: Low training error AND low validation error

---

## 4. Mathematical Foundations

### 4.1 Linear Algebra Prerequisites

**Vectors and Matrices:**

Our input embedding $x \in \mathbb{R}^{384}$ is transformed by weight matrices:

$$W_1 \in \mathbb{R}^{128 \times 384}, \quad W_2 \in \mathbb{R}^{64 \times 128}, \quad W_3 \in \mathbb{R}^{7 \times 64}$$

**Matrix Multiplication (Forward Pass):**

$$h_1 = W_1 \cdot x + b_1 \in \mathbb{R}^{128}$$

**Batch Processing:**

For batch of $N$ samples: $X \in \mathbb{R}^{N \times 384}$

$$H_1 = X \cdot W_1^T + \mathbf{1}_N \cdot b_1^T \in \mathbb{R}^{N \times 128}$$

### 4.2 Calculus - Gradients

**Chain Rule (Backpropagation):**

For composed functions $f(g(x))$:
$$\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$

**Gradient of BCE Loss:**

$$\frac{\partial \mathcal{L}}{\partial z_k} = \sigma(z_k) - y_k = \hat{y}_k - y_k$$

**Gradient of Linear Layer:**

$$\frac{\partial \mathcal{L}}{\partial W} = h_{prev}^T \cdot \delta$$
$$\frac{\partial \mathcal{L}}{\partial b} = \delta$$

### 4.3 Probability

**Sigmoid as Probability:**

$$P(y=1|x) = \sigma(z) = \frac{1}{1+e^{-z}}$$

**Cross-Entropy as Negative Log-Likelihood:**

$$\mathcal{L} = -\log P(y|x) = -[y \log \hat{y} + (1-y) \log(1-\hat{y})]$$

### 4.4 Cosine Similarity (Answer Evaluation)

**File:** `backend/core/answer_evaluator.py`

$$\text{sim}(u, v) = \frac{u \cdot v}{||u|| \cdot ||v||} = \frac{\sum_i u_i v_i}{\sqrt{\sum_i u_i^2} \cdot \sqrt{\sum_i v_i^2}}$$

```python
class AnswerEvaluator:
    def evaluate(self, user_answer, expected_answer):
        # Encode both answers to embeddings
        embeddings = self.model.encode([user_answer, expected_answer])
        
        # Compute cosine similarity
        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
        
        # Scale to percentage
        score = max(0, min(100, int(similarity * 100)))
        return score
```

---

## 5. Implementation Deep Dive

### 5.1 Data Pipeline

**File:** `backend/ml/data/data_loader.py`

```python
# Step 1: Load raw data
raw_data = load_raw_data("interview_intents.json")
# Format: [{"text": "I work with React hooks", "label": ["React"]}, ...]

# Step 2: Multi-label binarization
mlb = MultiLabelBinarizer(classes=TOPIC_LABELS)
labels = mlb.fit_transform(raw_labels)
# ["React", "JavaScript"] â†’ [0, 0, 1, 1, 0, 0, 0]

# Step 3: Text â†’ Embedding
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(texts)  # Shape: (N, 384)

# Step 4: Train/Val/Test Split (70/15/15)
X_train, X_val, X_test = train_test_split(...)

# Step 5: Create DataLoaders
train_loader = DataLoader(IntentDataset(X_train, y_train), batch_size=16)
```

### 5.2 Training Configuration

```python
CONFIG = {
    "batch_size": 16,           # Mini-batch size
    "learning_rate": 0.001,     # Initial learning rate
    "epochs": 100,              # Maximum epochs
    "early_stopping_patience": 15,  # Stop after 15 epochs without improvement
    "dropout_rate": 0.3,        # 30% dropout probability
    "weight_decay": 1e-4,       # L2 regularization strength
    "threshold": 0.5,           # Multi-label prediction threshold
}
```

### 5.3 Model Saving/Loading

```python
# Saving
torch.save({
    "model_state_dict": model.state_dict(),
    "config": {
        "input_dim": 384,
        "hidden_dim_1": 128,
        "hidden_dim_2": 64,
        "output_dim": 7,
        "dropout_rate": 0.3
    },
    "label_names": TOPIC_LABELS,
    "optimizer": "Adam",
    "test_metrics": test_metrics,
    "trained_at": datetime.now().isoformat()
}, "intent_model.pth")

# Loading
checkpoint = torch.load("intent_model.pth")
model.load_state_dict(checkpoint["model_state_dict"])
```

---

## 6. Training Pipeline

### 6.1 Complete Training Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          TRAINING PIPELINE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  PHASE 1: DATA LOADING                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  â€¢ Load interview_intents.json (282 samples)                               â”‚
â”‚  â€¢ Multi-label binarization (7 classes)                                     â”‚
â”‚  â€¢ Generate 384-dim embeddings                                              â”‚
â”‚  â€¢ Split: 70% train / 15% val / 15% test                                   â”‚
â”‚                                                                             â”‚
â”‚  PHASE 2: MODEL ARCHITECTURE                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  â€¢ Input: 384 dimensions                                                    â”‚
â”‚  â€¢ Hidden 1: 128 neurons (ReLU + Dropout 0.3)                              â”‚
â”‚  â€¢ Hidden 2: 64 neurons (ReLU + Dropout 0.3)                               â”‚
â”‚  â€¢ Output: 7 neurons (Sigmoid)                                              â”‚
â”‚  â€¢ Total: 55,623 trainable parameters                                       â”‚
â”‚                                                                             â”‚
â”‚  PHASE 3: TRAINING (OPTIMIZER COMPARISON)                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ Train with Adam optimizer                                                â”‚
â”‚  â€¢ Train with SGD optimizer (momentum=0.9)                                  â”‚
â”‚  â€¢ Compare loss curves and F1 scores                                        â”‚
â”‚  â€¢ Select best performing optimizer                                         â”‚
â”‚                                                                             â”‚
â”‚  PHASE 4: EVALUATION                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚  â€¢ Test set evaluation                                                      â”‚
â”‚  â€¢ Metrics: Accuracy, Precision, Recall, F1                                 â”‚
â”‚  â€¢ Per-label accuracy analysis                                              â”‚
â”‚                                                                             â”‚
â”‚  PHASE 5: DEPLOYMENT                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚  â€¢ Save model to intent_model.pth                                           â”‚
â”‚  â€¢ Load in IntentPredictor for inference                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Training Command

```bash
python backend/ml/training/train_intent_model.py
```

### 6.3 Expected Output

```
============================================================
PHASE 1: DATA LOADING
============================================================
[DATA] Loaded 282 samples from interview_intents.json
[DATA] Label distribution:
       Java: 50 samples
       Python: 50 samples
       JavaScript: 35 samples
       React: 30 samples
       SQL: 35 samples
       Machine_Learning: 40 samples
       Deep_Learning: 42 samples

[EMBEDDING] Encoding 282 texts...
[SPLIT] Train: 197 | Val: 42 | Test: 43

============================================================
PHASE 3: TRAINING (OPTIMIZER COMPARISON)
============================================================
Training with Adam
Epoch [  1/100] | Train Loss: 0.6543 | Val Loss: 0.5821 | F1: 0.234
Epoch [ 10/100] | Train Loss: 0.2341 | Val Loss: 0.2156 | F1: 0.782
Epoch [ 50/100] | Train Loss: 0.0523 | Val Loss: 0.0789 | F1: 0.943
âœ… Training complete | Best Val Loss: 0.0654

============================================================
PHASE 5: FINAL EVALUATION (TEST SET)
============================================================
ğŸ† Best Optimizer: Adam

ğŸ“Š TEST SET RESULTS:
   Exact Match Accuracy: 0.9302
   Precision: 0.9456
   Recall: 0.9234
   F1 Score: 0.9344

ğŸ’¾ Model saved to backend/ml/models/saved/intent_model.pth
```

---

## 7. Evaluation Metrics

### 7.1 Multi-Label Classification Metrics

**File:** `backend/ml/training/train_intent_model.py`

```python
def compute_metrics(predictions, targets, threshold=0.5):
    # Convert logits to binary predictions
    preds = (torch.sigmoid(predictions) >= threshold).float()
    
    # EXACT MATCH ACCURACY
    # All 7 labels must match exactly
    exact_match = (preds == targets).all(dim=1).float().mean()
    
    # PER-SAMPLE PRECISION, RECALL, F1
    tp = (preds * targets).sum(dim=1)           # True Positives
    fp = (preds * (1 - targets)).sum(dim=1)     # False Positives  
    fn = ((1 - preds) * targets).sum(dim=1)     # False Negatives
    
    precision = (tp / (tp + fp + Îµ)).mean()     # Correct among predicted
    recall = (tp / (tp + fn + Îµ)).mean()        # Correct among actual
    f1 = 2 * precision * recall / (precision + recall + Îµ)
    
    return {"exact_match": exact_match, "precision": precision, 
            "recall": recall, "f1": f1}
```

### 7.2 Metric Definitions

**Exact Match Accuracy:**
$$\text{Acc}_{exact} = \frac{1}{N} \sum_{i=1}^N \mathbb{1}[\hat{y}^{(i)} = y^{(i)}]$$

All 7 labels must match exactly.

**Precision (per-sample averaged):**
$$\text{Precision} = \frac{1}{N} \sum_{i=1}^N \frac{TP_i}{TP_i + FP_i}$$

"Of all labels we predicted, how many were correct?"

**Recall (per-sample averaged):**
$$\text{Recall} = \frac{1}{N} \sum_{i=1}^N \frac{TP_i}{TP_i + FN_i}$$

"Of all actual labels, how many did we find?"

**F1 Score:**
$$F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$

Harmonic mean balances precision and recall.

---

## 8. Code-to-Concept Mapping

### Complete File-to-Syllabus Mapping

| File | Module(s) | Concepts |
|------|-----------|----------|
| `intent_classifier.py` | 1, 2, 6 | MLP architecture, ReLU, Sigmoid, Dropout |
| `train_intent_model.py` | 2, 3, 6 | Backpropagation, Adam/SGD, L2, Early Stopping |
| `data_loader.py` | 1 | Vectorization, Data preprocessing |
| `intent_predictor.py` | 1, 2 | Inference, Forward pass |
| `answer_evaluator.py` | 1 | Embeddings, Cosine similarity |
| `interview_controller.py` | - | Integration of ML components |

### Deep Learning Component Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEEP LEARNING IMPLEMENTATION SUMMARY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  âœ… IMPLEMENTED (White Box):                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚  â€¢ Custom MLP Architecture (384â†’128â†’64â†’7)                                    â”‚
â”‚  â€¢ Backpropagation via PyTorch autograd                                       â”‚
â”‚  â€¢ Adam & SGD Optimizer Comparison                                            â”‚
â”‚  â€¢ Dropout Regularization (p=0.3)                                             â”‚
â”‚  â€¢ L2 Regularization (weight_decay=1e-4)                                      â”‚
â”‚  â€¢ Early Stopping (patience=15)                                               â”‚
â”‚  â€¢ Multi-label Binary Classification                                          â”‚
â”‚  â€¢ Xavier Weight Initialization                                               â”‚
â”‚  â€¢ Learning Rate Scheduling                                                   â”‚
â”‚  â€¢ Comprehensive Metrics (Precision, Recall, F1)                              â”‚
â”‚                                                                               â”‚
â”‚  ğŸ”§ USED (Pre-trained):                                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚  â€¢ Sentence Transformers (all-MiniLM-L6-v2) for embeddings                   â”‚
â”‚  â€¢ Whisper for Speech-to-Text (audio processing)                              â”‚
â”‚                                                                               â”‚
â”‚  ğŸ“‹ PLANNED (Future):                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  â€¢ CNN for webcam proctoring (Module 4)                                       â”‚
â”‚  â€¢ Autoencoder for answer compression (Module 5)                              â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Viva Questions & Answers

### Module 1: Introduction to Deep Learning

**Q1: What is the difference between a Perceptron and a Neuron in your MLP?**
> A Perceptron uses a step function (binary output), while our MLP neurons use differentiable activations (ReLU, Sigmoid) enabling gradient-based learning.

**Q2: Why do you use ReLU in hidden layers but Sigmoid in the output layer?**
> ReLU ($\max(0,x)$) is computationally efficient and avoids vanishing gradients. Sigmoid squashes output to (0,1), perfect for multi-label probability interpretation.

**Q3: Explain how vectorization helps in your project.**
> Text â†’ 384-dim vector using Sentence Transformers. This creates a fixed-size, semantically meaningful input for our neural network. Similar texts have similar vectors.

### Module 2: Backpropagation

**Q4: Walk through one forward-backward pass in your training loop.**
> 1. Forward: Input (384) â†’ Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Logits (7)
> 2. Loss: BCEWithLogitsLoss computes error
> 3. Backward: `loss.backward()` computes gradients via chain rule
> 4. Update: `optimizer.step()` adjusts weights

**Q5: What happens if you forget `optimizer.zero_grad()`?**
> Gradients accumulate across batches. Weight updates become incorrect (sum of all batch gradients instead of current batch only).

### Module 3: Optimization

**Q6: Why does Adam perform better than SGD in your experiments?**
> Adam adapts learning rates per-parameter using first/second moment estimates. Our 384-dim embeddings have varying scales; Adam handles this automatically.

**Q7: What is the role of the learning rate scheduler?**
> `ReduceLROnPlateau` reduces LR when validation loss plateaus. Starts with fast learning, then fine-tunes near the optimum.

### Module 6: Regularization

**Q8: How does Dropout work during training vs. testing?**
> Training: Randomly zeros 30% of neurons, scales others by 1/(1-p). Testing: All neurons active (no dropout). This simulates ensemble averaging.

**Q9: What is early stopping and why is it important?**
> Stop training when validation loss stops improving (15 epochs patience). Prevents overfitting to training noise. We restore the best model checkpoint.

**Q10: Explain the bias-variance tradeoff in your model design.**
> Our model (55K params) is constrained enough to avoid high variance (overfitting) but expressive enough to avoid high bias (underfitting). Regularization further controls variance.

### General Deep Learning

**Q11: What makes this a "white box" project vs. just using APIs?**
> We designed the architecture (layer sizes, activations), implemented training loops, compared optimizers, and applied regularization techniques. We understand every mathematical operation, not just API calls.

**Q12: How do you handle multi-label classification differently from multi-class?**
> Multi-class: Softmax (outputs sum to 1) + CrossEntropyLoss. Multi-label: Sigmoid per class (independent probabilities) + BCEWithLogitsLoss. One sample can have multiple labels.

---

## 10. References

### Academic
1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature.
3. Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic Optimization. ICLR.

### Libraries Used
- PyTorch: https://pytorch.org/
- Sentence Transformers: https://www.sbert.net/
- Scikit-learn: https://scikit-learn.org/

### Model Resources
- MiniLM: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
- Whisper: https://openai.com/research/whisper

---

## Appendix A: Running the Project

### Installation

```bash
# Clone and setup
cd "DL Project"
pip install -r requirements.txt
```

### Training the Model

```bash
python backend/ml/training/train_intent_model.py
```

### Running Inference

```python
from backend.ml.training.intent_predictor import predict_topics

topics = predict_topics("I work with React hooks and JavaScript")
# Output: ['React', 'JavaScript']
```

### Full Interview

```bash
python backend/core/interview_controller.py
```

---

## Appendix B: Dataset Summary

**File:** `backend/ml/data/interview_intents.json`

| Topic | Samples | Example |
|-------|---------|---------|
| Java | 50 | "java inheritance polymorphism encapsulation abstraction" |
| Python | 50 | "pandas numpy basics" |
| JavaScript | 35 | "javascript async await promises" |
| React | 30 | "react functional components hooks" |
| SQL | 35 | "select join where clause" |
| Machine_Learning | 40 | "linear regression classification" |
| Deep_Learning | 42 | "neural networks layers" |

**Multi-label Examples:**
- "python used for deep learning training" â†’ [Python, Deep_Learning]
- "java backend sql react frontend" â†’ [Java, SQL, React]

---

## Appendix C: Project Structure

```
DL Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ answer_evaluator.py      # Cosine similarity evaluation
â”‚   â”‚   â”œâ”€â”€ interview_controller.py  # Main interview logic
â”‚   â”‚   â””â”€â”€ question_bank.py         # Question repository
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_loader.py       # Data preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ interview_intents.json  # Training data
â”‚   â”‚   â”‚   â””â”€â”€ processed/           # Cached embeddings
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_classifier.py # MLP architecture (Module 1, 2, 6)
â”‚   â”‚   â”‚   â””â”€â”€ saved/
â”‚   â”‚   â”‚       â””â”€â”€ intent_model.pth # Trained model
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”‚   â”œâ”€â”€ train_intent_model.py # Training loop (Module 2, 3, 6)
â”‚   â”‚   â”‚   â””â”€â”€ intent_predictor.py   # Inference wrapper
â”‚   â”‚   â””â”€â”€ plots/
â”‚   â”‚       â””â”€â”€ training_curves.png   # Optimizer comparison
â”‚   â””â”€â”€ resume/
â”‚       â””â”€â”€ question_generator.py     # GPT integration
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html                    # Web interface
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ scenarios.py                  # Test cases
â”œâ”€â”€ requirements.txt                  # Dependencies
â”œâ”€â”€ README.md                         # Project overview
â””â”€â”€ dl.md                            # THIS DOCUMENT
```

---

**Document Version:** 1.0  
**Last Updated:** January 2026  
**Author:** Student Project Team

---

*This document demonstrates comprehensive understanding and implementation of Deep Learning concepts as per the course syllabus. All code is original and available in the project repository for verification.*
